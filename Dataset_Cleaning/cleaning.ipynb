{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Feature Engineering of the Emory RCC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Take\n",
    "1. Load dataset into pandas dataframe\n",
    "2. Drop unnecessary or 100% empty columns\n",
    "3. Parse dates \n",
    "4. Decouple compounded features\n",
    "5. Transform data\n",
    "6. Drop single value columns\n",
    "7. Clean dataframe dtypes\n",
    "8. Handle outliers\n",
    "9. Imputation\n",
    "10. Feature selection\n",
    "11. Sample dropping\n",
    "12. Scale and Normalize\n",
    "13. Clean col names\n",
    "14. Save cleaned CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 12:25:23.620633: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 12:25:24.734264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Patient ID', 'Patient MRN', 'Last Name', 'First Name', 'Patient SSN', 'Date of Birth', 'Fistula ']\n",
      "[]\n",
      "(1881, 408)\n"
     ]
    }
   ],
   "source": [
    "dateparse = lambda x: datetime.strptime(x, '%m/%d/%Y')\n",
    "df = pd.read_csv('../Data Sources/DO NOT MODIFY/US_RCC_Database.csv', parse_dates=True, date_format='%m/%d/%Y')\n",
    "# Drop all rows and columns that are completely empty\n",
    "empty_cols = [col for col in df.columns if df[col].isnull().all()]\n",
    "empty_rows = [row for row in df.index if df.iloc[row].isnull().all()]\n",
    "print(empty_cols)\n",
    "print(empty_rows)\n",
    "df.drop(labels=empty_cols, axis=1, inplace=True)\n",
    "df.drop(index=empty_rows, axis=0, inplace=True)\n",
    "data_shape = df.shape\n",
    "print(data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run a simple df.shape, this dataset has a total of 415 features and 1881 samples. That is a lot a features for the model to draw from, so we need to make sure they are all relevant to the task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few columns with duplicate names, we need to go through and adjust these to be more descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: First result is 'Primary v. Recurrence', not relevant\n",
    "dup_cols = df.columns[df.columns.str.contains(\"\\.\")].to_list()[1:]\n",
    "new_names = ['Date of Initiation_NeoadjChemoRad',\n",
    "            'Date of Completion_NeoadjChemoRad',\n",
    "            'Number of Mesorectal Nodes by MRI_Post',\n",
    "            'Number of Mesorectal Nodes by ERUS_Post',\n",
    "            'Number of Retroperitoneal Nodes by CT_Post',\n",
    "            'Number of Retroperitoneal Nodes by MRI_Post',\n",
    "            'Number of Retroperitoneal Nodes by PET-CT_Post',\n",
    "            'Number of Pelvic Nodes on CT_Post',\n",
    "            'Number of Pelvic Nodes on MRI_Post',\n",
    "            'Number of Pelvic Nodes on PET-CT_Post',\n",
    "            'Involvement of Pelvic Sidewall_Post',\n",
    "            'Distal Circumferential or Radial Margin (mm)_Post',\n",
    "            'Sphincter Involvement_Post',\n",
    "            'Invasion into Reproductive Organs_Post',\n",
    "            'Invasion into Bladder_Post',\n",
    "            'Invasion into Sacrum_Post',\n",
    "            'Invasion of Sacral Nerve Roots_Post',\n",
    "            'Involvement of Pelvic Sidewall_Op',\n",
    "            'Omental Flap to Pelvis_Op_APR',\n",
    "            'T-Stage_Recurrence',\n",
    "            'N-Stage_Recurrence',\n",
    "            'Type of Operation of Rectal Tumor_Recurrence',\n",
    "            'Distal Margin Distance (cm)_Recurrence',\n",
    "            'Radial Margin Distance (mm)_Recurrence',\n",
    "            'Date of Diagnosis_PostOp_Leak',\n",
    "            'Acute Renal Failure_PostOp',\n",
    "            'Date of Discharge_Readmission',\n",
    "            'Date of Initiation_AdjChemo',\n",
    "            'Date of Completion_AdjChemo',\n",
    "            'Date of Initiation_AdjChemoRad',\n",
    "            'Date of Completion_AdjChemoRad',\n",
    "            'Radiation Technique_AdjChemoRad',\n",
    "            'Date of Initiation_AdjRad',\n",
    "            'Date of Completion_AdjRad',\n",
    "            'Sacrum_Recurrence_Locoregional',\n",
    "            'Bladder_Recurrence_Locoregional',\n",
    "            'Seminal Vesicles_Recurrence_Locoregional',\n",
    "            'Prostate_Recurrence_Locoregional',\n",
    "            'Vagina_Recurrence_Locoregional',\n",
    "            'Ureter_Recurrence_Locoregional',\n",
    "            'Grey (Gy)_Recurrence_Rad',\n",
    "            'Ablation_Recurrence']\n",
    "\n",
    "df.rename(columns=dict(zip(dup_cols, new_names)), inplace=True)\n",
    "assert df.shape == data_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop unnecessary or empty columns\n",
    "First, to aid in the cleaning process, we'll extract the number of missing and unique (if discrete) values for each feature and save to a file for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categorical features:  346\n",
      "# unique values by categorical features: \n",
      "% missing values by column: \n",
      " Radiotherapy Complications                 0.999468\n",
      "Complication after Stenting                0.999468\n",
      "Intervention for Peritoneal Perforation    0.997873\n",
      "Radiation Technique_AdjChemoRad            0.996810\n",
      "Necrosis (%)                               0.996279\n",
      "                                             ...   \n",
      "Previous Diagnosis of Prostate Cancer      0.000000\n",
      "Previous Diagnosis of GYN Cancer           0.000000\n",
      "Disseminated Cancer                        0.000000\n",
      "Prior PELVIC Radiation                     0.000000\n",
      "Database ID                                0.000000\n",
      "Length: 408, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"# categorical features: \", len(df.select_dtypes(include=['object']).columns.tolist()))\n",
    "print(\"# unique values by categorical features: \")\n",
    "print(\"% missing values by column: \\n\", df.isnull().sum().div(data_shape[0]).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnecessary columns (a * indicates that the column is a duplicate name and needs to be renamed):\n",
    "- Watch and Wait Protocol\n",
    "- Adjuvant Radiotherapy\n",
    "- Grey (Gy)\n",
    "- Radiation Technique\n",
    "- Date of Initiation-Radiotherapy *\n",
    "- Date of Completion-Radiotherapy *\n",
    "- Radiotherapy Complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Database ID', 'Gender', 'Age', 'Race', 'Height (cm)', 'Weight (kg)',\n",
      "       'BMI', 'Zip Code', 'Health Insurance', 'ASA Class',\n",
      "       ...\n",
      "       'Ovary', 'Bone', 'Distant LNs', 'Chemotherapy', 'Chemotherapy Regimen',\n",
      "       'Radiation', 'Grey (Gy)_Recurrence_Rad', 'Ablation_Recurrence',\n",
      "       'Surgery', 'COMMENTS'],\n",
      "      dtype='object', length=408)\n"
     ]
    }
   ],
   "source": [
    "empty_cols = df.columns[df.isnull().mean() < 1]\n",
    "df = df[empty_cols]\n",
    "print(empty_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_cols = ['Watch and Wait Protocol', 'Adjuvant Radiotherapy', 'Grey (Gy)', 'Radiation Technique', 'Date of Initiation-Radiotherapy', 'Date of Completion-Radiotherapy', 'Radiotherapy Complications']\n",
    "df = df.drop(columns=drop_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse dates\n",
    "In this example, I will be selecting each column by hand that contains a date format. This is probably not the best way to do this, but I would like to conserve the naming conventions of the dataset and I'm too lazy to figure out how to do this automatically via Pandas. Instead of parsing dates into day, month, and year columns, each column will be replaced with the # of days since the date of diagnosis. This is done to avoid the model drawing inferences between past time frames and overfitting to the training dataset. This also requires that 'Date of Diagnosis' be dropped.\n",
    "### It is probably much better to use pd.to_timedelta for this, I didn't notice it existed until later\n",
    "### Colmuns with date data (a * indicates that the column is a duplicate name and needs to be renamed):\n",
    "- Date of Diagnosis\n",
    "- Date of First Oncologic Consultation\n",
    "- Date of Biopsy\n",
    "- Date of Initiation-Neoadjuvant Chemotherapy *\n",
    "- Date of Completion-Neoadjuvant Chemotherapy *\n",
    "- Date of Initiation-Neoadjuvant Chemoradiation *\n",
    "- Date of Completion-Neoadjuvant Chemoradiation *\n",
    "- Date of Surgery\n",
    "- Date of Diagnosis-Anastomic Leak *\n",
    "- Date of Intervention-Anastomic Leak *\n",
    "- Date of Drain Removal\n",
    "- Date of Ileostomy Reversal \n",
    "- Date of Reoperation\n",
    "- Date of ICU Admission\n",
    "- Date of ICU Discharge\n",
    "- Date of Discharge\n",
    "- Date of Readmission\n",
    "- Date of Discharge-Readmission *\n",
    "- Date of Initiation-Adjuvant Chemotherapy *\n",
    "- Date of Completion-Adjuvant Chemotherapy *\n",
    "- Date of Initiation-Adjuvant Chemoradiation *\n",
    "- Date of Completion-Adjuvant Chemoradiation *\n",
    "- Date of Last Follow-up/Death\n",
    "- Date of Death\n",
    "- Date of Recurrence\n",
    "- Date Rectal Stent Placed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling dates BEFORE date of diagnosis\n",
    "In this dataset, there are features where the date values precede the date of diagnosis. This causes issues when we use the date of diagnosis as the 'origin' date as the distance between the two could be negative, the current solution is to just let the values be negitive. If this could be attributed to a loss in accuracy, this solution could be replaced with simply dropping dates occurring before the origin or changing the origin date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "date_cols = ['Date of First Oncologic Consultation', 'Date of Biopsy', 'Date of Initiation-Neoadjuvant Chemotherapy', 'Date of Initiation-Neoadjuvant Chemoradiation', 'Date of Surgery', 'Date of Diagnosis-Anastomic Leak', 'Date of Intervention-Anastomic Leak', 'Date of Reoperation', 'Date of ICU Admission', 'Date of Initiation-Adjuvant Chemotherapy', 'Date of Initiation-Adjuvant Chemoradiation', 'Date of Last Follow-up/Death', 'Date of Recurrence', 'Date Rectal Stent Placed']\n",
    "diagnosis_dates = df['Date of Diagnosis']\n",
    "for col in date_cols:\n",
    "    values = df[col]\n",
    "    null_values = values.isnull()\n",
    "    index = df.columns.get_loc(col)\n",
    "    col_tokens = col.split()\n",
    "    new_name = 'Days to'\n",
    "    for token in col_tokens[2:]:\n",
    "        new_name = new_name + ' ' + token\n",
    "    new_values = []\n",
    "    count = 0\n",
    "    null_vals = values.isnull()\n",
    "    for value in values:\n",
    "        if null_vals.get(count) == False:\n",
    "            diagnosis = diagnosis_dates.get(count).split('/')\n",
    "            date_to = value.split('/')\n",
    "            d1 = date(int(date_to[2]), int(date_to[0]), int(date_to[1]))\n",
    "            d2 = date(int(diagnosis[2]), int(diagnosis[0]), int(diagnosis[1]))\n",
    "            new_values.append((d1 - d2).days)\n",
    "        else:\n",
    "            new_values.append(None)\n",
    "        count += 1\n",
    "    df.insert(index, new_name, new_values, allow_duplicates=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(date_cols)\n",
    "df = df.drop(date_cols, axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dates = ['Date of Diagnosis', 'Date of Completion-Neoadjuvant Chemotherapy', 'Date of Completion-Neoadjuvant Chemoradiation', 'Date of Drain Removal', 'Date of Ileostomy Reversal', 'Date of ICU Discharge', 'Date of Discharge', 'Date of Readmission', 'Date of Discharge-Readmission', 'Date of Completion-Adjuvant Chemotherapy', 'Date of Completion-Adjuvant Chemoradiation', 'Date of Death']\n",
    "df = df.drop(columns=drop_dates)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decouple compounded features\n",
    "For example, a list of complications from a treatment. These should be split into multiple columns and one-hot encoded.\n",
    "### Columns to be parsed (a * indicates that the column is a duplicate name and needs to be renamed): \n",
    "- Complication During Neoadjuvant Treatment (Side Note: This should be categorical, 'Yes' values should be removed and Imputated with median or similar)\n",
    "- Type of Intraoperative Complication \n",
    "- Organs Invaded\n",
    "- Neoadjuvant Chemo Regimen (Side Note: This feature needs to be adaquitely cleaned, some of the values are not consistent or straightforward)\n",
    "- Adjuvant Chemo Regimen (Side Note: This feature needs to be adaquitely cleaned, some of the values are not consistent or straightforward)\n",
    "- Chemotherapy Regimen-Recurrence (Side Note: This feature needs to be adaquitely cleaned, some of the values are not consistent or straightforward) *\n",
    "### Processes\n",
    "With many unique values, binning/n-grams will need to be practiced to handle misspellings or other mistakes. This will be done by splitting each value into tokens and compliling a list of unique values to inspect and bin. Many of these features also have secondary columns that indicate whether or not the specific features are present. For example, a Chemotherapy field with yes or no values that correspond to the entries in the Chemotherapy Regimen-Recurrence column. These will need to be removed to avoid confusing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnosis at readmission was discovered to need binning during dtype cleaning. Since binning is very time intensive with a dataset like this and the Nan count in the feature is 1358 after dtype cleaning pass 1, we will bin this iff it makes it past feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = df['Diagnosis at Readmission'].isna().sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerator = 0\n",
    "unique_tokens = []\n",
    "for entry in therapy_complications:\n",
    "    if null_values.get(enumerator) == False:\n",
    "        if entry == 'No':\n",
    "            new_values.append('No')\n",
    "        elif entry == 'Yes':\n",
    "            new_values.append(\"Yes\")\n",
    "        else:\n",
    "            new_values.append('Yes')\n",
    "        entry = entry.replace(';', ',')\n",
    "        entry = entry.replace(', and', ',')\n",
    "        tokens = entry.split(', ')\n",
    "        trigger = False\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            for u in unique_tokens:\n",
    "                if token == u.lower():\n",
    "                    trigger = True\n",
    "            if (not trigger) and (token != 'no') and (token != 'yes') and (token != 'per notes'):\n",
    "                unique_tokens.append(token)\n",
    "            trigger = False\n",
    "    else:\n",
    "        new_values.append('No')\n",
    "    enumerator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens(col_name, split_indicators, df):\n",
    "    index = df.columns.get_loc(col_name)\n",
    "    values = df[col_name]\n",
    "    null_values = values.isnull()\n",
    "    enumerator = 0\n",
    "    unique_tokens = []\n",
    "    for value in values:\n",
    "        if null_values.get(enumerator) == False:\n",
    "            for indicator in split_indicators:\n",
    "                value = value.replace(indicator, ',')\n",
    "            tokens = value.split(',')\n",
    "            trigger = False\n",
    "            for token in tokens:\n",
    "                token = token.lower()\n",
    "                for u in unique_tokens:\n",
    "                    if token == u.lower():\n",
    "                        trigger = True\n",
    "                if (not trigger):\n",
    "                    unique_tokens.append(token)\n",
    "                trigger = False\n",
    "        enumerator += 1\n",
    "    return unique_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning method\n",
    "Each column to be binned has its own dict with the keys being the bins to place the values and the items being an array of unique values that belong in that bin. Chemotherapies will be one hot encoded and given a 'swapped to' column containing the secondary regimen bin to let the model know that it had to be changed. All other features will have their unique values concatenated into their respective bin so the model can draw inferences between the magnitude of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin(col_name, bin_dict, df, chemo_indicator = False):\n",
    "    col_data = df[col_name]\n",
    "    null_entries = col_data.isnull()\n",
    "    col_index = df.columns.get_loc(col_name)\n",
    "    chemo_indicies = []\n",
    "    chemo_secondary = []\n",
    "    key_num = 0\n",
    "    for key in bin_dict:\n",
    "        new_values = []\n",
    "        items = bin_dict[key]\n",
    "        iterator = 0\n",
    "        for data in col_data:\n",
    "            if (key_num == 0) and chemo_indicator:\n",
    "                chemo_indicies.append(None)\n",
    "                chemo_secondary.append('None')\n",
    "            new_values.append(0)\n",
    "            if null_entries.get(iterator) == False:\n",
    "                for item in items:\n",
    "                    if item in data.lower():\n",
    "                        if chemo_indicator:\n",
    "                            therapy_index = data.lower().find(item)\n",
    "                            if chemo_indicies[iterator] == None:\n",
    "                                chemo_indicies[iterator] = therapy_index\n",
    "                            elif chemo_indicies[iterator] > therapy_index:\n",
    "                                chemo_secondary[iterator] = key\n",
    "                                chemo_indicies[iterator] = therapy_index\n",
    "                            new_values[iterator] = 1\n",
    "                        else:\n",
    "                            scale = 1\n",
    "                            if 'grade ' in data.lower():\n",
    "                                grade_index = data.lower().find('grade')\n",
    "                                grade_val = grade_index + 6\n",
    "                                scale = scale * (int(data[grade_val]))\n",
    "                            new_values[iterator] = int(new_values[iterator]) + (1 * scale)\n",
    "            iterator += 1\n",
    "        if chemo_indicator:\n",
    "            df.insert(col_index, col_name + '-' + key, new_values, allow_duplicates = False)\n",
    "        else:\n",
    "            df.insert(col_index, key, new_values, allow_duplicates = False)\n",
    "        key_num += 1\n",
    "    if chemo_indicator:\n",
    "        df.insert(col_index, col_name+'-Secondary Regimen', chemo_secondary, allow_duplicates=False)\n",
    "    df = df.drop(columns = col_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Therapy Complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_tokens = get_unique_tokens('Complication During Neoadjuvant Treatment', ['; ', ', and', ', ', ',  '], df)\n",
    "therapy_dict = {\n",
    "    'Cardiovascular': ['nstemi', 'coronary vasospasm', 'vascular spasm of the fingers', 'thrombophlebitis', 'dvt', 'atrial fibrillation'],\n",
    "    'Blood/Bone Marrow': ['thrombocytopenia', 'low platelets', 'induced thromboctopenia', 'anemia', 'low wbc', 'neutropenia', 'sepsis', 'cytopenia', 'necrosis', 'bacteremia'],\n",
    "    'Constitutioinal Symptoms': ['nose and lip bleeding', 'suicidal ideations', 'anorexia', 'sirs', 'dehydration', 'failure to thrive', 'fatigue', 'cold intolerance', 'night sweats', 'cold sensitivity', 'hair loss', 'dizziness', 'nausea', 'vomitting', 'tongue and throat sensitivity', 'alopecia', 'allergic reaction', 'abdominal cramping'],\n",
    "    'Dermatology': ['periananl skin', 'rash', 'hand foot syndrome', 'gout flare', 'hand and foot', 'palmar-plantarerythrodysesthesia', 'dermatitis', 'cellulitis', 'pruritis', 'skin toxicity', 'skin erythema', 'desquamation', 'skin erythema with desquamation', 'skin reaction', 'skin irrritation', 'perineal and vaginal skin irritation', 'skin changes', 'pruritus', 'skin desquamation', 'hand-foot syndrome', 'skin issues', 'pilonidal abscess'],\n",
    "    'Reproductive': ['genital herpes', 'erectile dysfunction', 'menorrhagia', 'vagina'], \n",
    "    'Pulmonary': ['pneumothorax', 'pe', 'pulmonary hypertension', 'tracheal bronchitis'], \n",
    "    'Renal': ['ureter obstruction', 'acute kidney injury', 'aki'],\n",
    "    'Pain': ['low backache', 'pain', 'perirectal burning', 'pneumonia', 'thrombophlebitis', 'gout', 'cramping'],\n",
    "    'Neurology': ['numbness in hands', 'neuropathy', 'grade 1 taste alteration', 'dysuria', 'numb fingers', 'chemo brain'], \n",
    "    'Gastrointestinal': ['diarrhea', 'urinary freq', 'constipation', 'esophageal spasm', 'dysuria', 'urinary incontinence', 'fecal obstruction', 'gi bleed', 'colitis', 'bowel obstruction', 'incontinence', 'sbo', 'dysphagia', 'mucositis', 'crohn', 'pyroderma gangrenosum', 'hernia', 'ileitis', 'mouth sores', 'dysuria', 'mouth ulcers', 'enteritis', 'pneumatosis', 'diverticulitis', 'uti', 'urinary obstructive', 'mucousitis', 'stomatitis'],\n",
    "    'Rectum': ['proctitis', 'perirectal burning', 'rectal pain' 'dermatitis and superimposed cellulitis of the periananl skin', 'anorectal pain', 'fistula', 'perineal and inguinal abscess', 'rectal bleeding', 'rectal irritation', 'rectal pain and bleeding', 'anal pruritis/irritation', 'perianal irritation', 'hemorrhoids', 'perirectal abscess', 'prostatitis']\n",
    "}\n",
    "df = bin('Complication During Neoadjuvant Treatment', therapy_dict, df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operative Complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_vals = get_unique_tokens('Type of Intraoperative Complication ', [','], df)\n",
    "operative_dict = {\n",
    "    'GI Mesenteric Vessel Injury': ['mesenteric injury', 'colonic ischemia requiring excision of colon to hepatic flexure', 'ischemia noted without obvious vascular injury/disruption. distal transverse and descending colon were resected along with appendectomy', 'colorectal anastomosis', 'ischemia to colon requiring deloters procedure'],\n",
    "    'GI Rectal Injury': ['bladder injury - Urinary tract hole in rectal stump after firing stapler', 'tear in rectum distal to anastomosis', 'entered the vagina and the rectal lumen accidentally', 'rectal injury', 'proctotomy'],\n",
    "    'GI Anastomotic Leak': ['anastamotic defect', 'anastomotic leak requiring dli', 'staples did not hold rectal stump closed', 'anatosmotic leak repaired', 'positive leak test'],\n",
    "    'Vaginal Injury': ['vaginal injury', 'hole created in posterior vaginal wall', 'entered the vagina and the rectal lumen accidentally', 'colpotomy'],\n",
    "    'GI Small Bowel Injury': ['small bowel cautery injury', 'bowel injury', 'small bowel enterotomy'],\n",
    "    'Prostate Injury': ['tear in rectum distal to anastomosis'],\n",
    "    'Bleeding-Operative': ['bleeding', 'acidosis'],\n",
    "    'Urinary Tract Injury': ['bladder injury', 'ureteral injury', 'coagulopathy - abd left open and taken to icu', 'urethral injury'],\n",
    "    'Spleen Injury': ['splenic capsule tear']\n",
    "}\n",
    "df = bin('Type of Intraoperative Complication ', operative_dict, df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organs Invaded\n",
    "Need to determine relationship to Other Organ Involvement col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "organ_tokens = get_unique_tokens('Organs Invaded', [', ', ' and '], df)\n",
    "organ_dict = {\n",
    "    'Reproductive-Invaded': ['vagina', 'vas deferens', 'cervix', 'prostate', 'seminal vesticles', 'uterus', 'fallopian tube', 'ovaries', 'scrotum', 'ovary'],\n",
    "    'Colon': ['colon'],\n",
    "    'Anus': ['anus', 'peri-anal skin with metastatic lesion'],\n",
    "    'Urinary Tract': ['urinary', 'periureteral soft tissue', 'bladdder', 'ureter', 'bladder'],\n",
    "    'Nodes': ['nodes'],\n",
    "    'Sacrum-Invaded': ['pre-sacral tissue', 'sacrum'],\n",
    "    'Liver-Invaded': ['liver'],\n",
    "    'Pelvic Bone': ['pelvic wall', 'obturator side wall', 'coccyx', 'pelvis', 'pelvic side wall', 'perisacral soft tissue'],\n",
    "    'Pelvic Floor': ['pelvic floor muscle'],\n",
    "    'Omentum': ['omentum'],\n",
    "    'Spleen': ['spleen'],\n",
    "    'Peritoneum': ['peritoneum'],\n",
    "    'Soft Tissue': ['ab wall', 'buttocks'],\n",
    "    'Small Bowel-Invaded': ['small bowel', 'ileum'],\n",
    "    'Stomach': ['stomach'],\n",
    "    'Mesentery': ['mesentary', 'mesentery'],\n",
    "    'Vessels': ['iliac vessels']\n",
    "}\n",
    "df = bin('Organs Invaded', organ_dict, df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemo Regimens\n",
    "Make a seperate feature that indicates if a patient had to switch regimens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regimen_cols = ['Neoadjuvant Chemo Regimen', 'Adjuvant Chemo Regimen', 'Chemotherapy Regimen-Recurrence']\n",
    "unique_regimens = []\n",
    "for col in regimen_cols:\n",
    "    regimen_tokens = get_unique_tokens(col, [', ', ' + ', '+', ' --> ', '--> ', ' -->', '-->', ' -> ', ' ->', ';', '; ', ' then ', '/', ', switched to ', ', and ', ' ,', ' and '], df)\n",
    "    for token in regimen_tokens:\n",
    "        unique_regimens.append(token)\n",
    "tokens = []\n",
    "for value in unique_regimens:\n",
    "    trigger = False\n",
    "    for token in tokens:\n",
    "        if (value == token.lower()) or (value == 'no'):\n",
    "            trigger = True\n",
    "    if (not trigger):\n",
    "        tokens.append(value)\n",
    "    trigger = False\n",
    "chemo_dict = {\n",
    "    'Folfox': ['xelox', 'folfox', 'oxaliplatin', 'flox', 'capox', 'folfirinox', 'capeox', 'xeliri', 'foflox', 'xeleri'],\n",
    "    '5FU': ['xeloda', 'capecitabine', '5-fu-based', '5-fu', 'leucovorin', '5fu', 'leocovorin', 'leukovorin', 'cabecitabine'],\n",
    "    'Biologic': ['avastin', 'bbi', 'panitumumab', 'pembro', 'nivolumab', 'erbutux', 'cetuximab', 'erbitux', 'bev', 'cituximab', 'vectibix', 'xl888 trial', 'panitumaumab', 'lonsurf', 'ipilimumab', 'avastn', 'cetuximab', 'avastatin', 'avasrin'],\n",
    "    'Folfiri': ['irinotecan', 'folfiri', 'capiri', 'bi5013', 'xeliri', 'folriri'],\n",
    "    'Interleukin': ['nktr-214 (pegylated il-2)'],\n",
    "    'Alt Chemo': ['etoposide', 'camtosar', 'cisplatin', 'carboplatin', 'carboxyplatin', 'mmc', 'other']\n",
    "}\n",
    "for i in regimen_cols:\n",
    "    df = bin(i, chemo_dict, df, chemo_indicator=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform data\n",
    "Some data in this dataset needs to be transformed to fit the same scale as other, related features.\n",
    "### Columns to transform with units from/to in parenthesis (a * indicates that the column needs to be renamed): \n",
    "- Duration of Neoadjuvant Chemo-Days (Months/Days) *\n",
    "- Duration of Neoadjuvant ChemoXRT-Days (Months/Days) *\n",
    "- Adjuvant Chemo Duration-Days (Weeks/Days) *\n",
    "- Adjuvant Chemoradiation Duration-Days (Weeks/Days) *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#running this cell more than once WILL result in CORRUPTED VALUES, restart kernel before running\n",
    "\n",
    "months_to_days = ['Duration of Neoadjuvant Chemo-Days', 'Duration of Neoadjuvant ChemoXRT-Days']\n",
    "for feat in months_to_days:\n",
    "    df[feat] = df[feat].infer_objects()\n",
    "    df[feat] = df[feat].multiply(30)\n",
    "\n",
    "weeks_to_days = ['Adjuvant Chemo Duration-Days', 'Adjuvant Chemoradiation Duration-Days']\n",
    "for feat in weeks_to_days:\n",
    "    df[feat] = df[feat].infer_objects()\n",
    "    df[feat] = df[feat].multiply(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Drop single value columns\n",
    "Just like it sounds, these are columns that only contain a single value. These will only serve to distract our model since it cannot draw an inference from a static value, so we need to drop them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "fs = FeatureSelector(data = df)\n",
    "fs.identify_single_unique()\n",
    "print(fs.ops['single_unique'])\n",
    "fs.plot_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=fs.ops['single_unique'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean dataframe dtypes\n",
    "Transforming months to days in step 5 caused a lot of errors that needed to be fixed since a few samples had strings instead of integers which led to issues when multiplying. This means that we need to go back through the csv and identify any columns that have values with dtypes that are not uniform accross all samples. This will help moving forward when we have to deal with outliers and imputation. This really should have been one of the first steps, but as we work through this we are modifying the main dataset so it will apply to all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The column 'Distance from Sphincters' was misclassified as an Object but contains only floats, will have to cast it manually...\n",
    "### 'Type of Anastomosis' contains integers/floats but is a categorical col..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.infer_objects()\n",
    "df['Distance from Sphincters'] = pd.to_numeric(df['Distance from Sphincters'], errors = 'coerce')\n",
    "df['Type of Anastomosis'] = df['Type of Anastomosis'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Distal Circumferential or Radial Margin (mm)\n",
    "Split between 0 and >0. For convience, all '>0' values in the dataset have been changed to '1.0' to avoid an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Distal Circumferential or Radial Margin (mm)'] = pd.cut(df['Distal Circumferential or Radial Margin (mm)'], [-1000000000, 0, 1000000000], labels = ['0', '>0'])\n",
    "df['Distal Circumferential or Radial Margin (mm).1'] = pd.cut(df['Distal Circumferential or Radial Margin (mm).1'], [-1000000000, 0, 1000000000], labels = ['0', '>0'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling additional margins\n",
    "- Radial margin should be grouped into >=3 \n",
    "- Distal margin should be grouped into >=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Radial Margin Distance (mm)'] = pd.cut(df['Radial Margin Distance (mm)'], [-1000000000, 3, 1000000000],  right = False, labels = ['<3', '>=3'], include_lowest = True)\n",
    "df['Radial Margin Distance (mm).1'] = pd.cut(df['Radial Margin Distance (mm).1'], [-1000000000, 3, 1000000000], right = False, labels = ['<3', '>=3'], include_lowest = True)\n",
    "\n",
    "df['Distal Margin Distance (cm)'] = pd.cut(df['Distal Margin Distance (cm)'], [-1000000000, 1, 1000000000], right = False, labels = ['<1', '>=1'], include_lowest = True)\n",
    "df['Distal Margin Distance (cm).1'] = pd.cut(df['Distal Margin Distance (cm).1'], [-1000000000, 1, 1000000000], right = False, labels = ['<1', '>=1'], include_lowest = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to categorical dtype\n",
    "Now that all numeric values have been converted to numeric dtypes, the only thing we need to do now is to convert all remaining Objects to categorical. This will help us do outlier correction and imputation without going over the dataset by hand again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cat = df.select_dtypes(include = 'object')\n",
    "for col in to_cat.columns:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are first going to infer dtypes for all objects in pandas so we can see and drop the samples that keep pandas from inferring the proper dtypes\n",
    "dtypes = df.dtypes\n",
    "dtypes.to_csv('inferred_dtypes.csv')\n",
    "df.to_csv('US_RCC_Dataset_dtype_cleaning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning log:\n",
    "Below is a list of modifications made to the main dataset. It is extensive, but necessary in case one or more introduces bias into the model and we need to undo it. In cases where samples were dropped if they contained a certain value or qualifier, no more than about 10 samples were dropped at a time. Keep in mind that undoing certain edits below may cause a sample to raise an error in another column/feature.\n",
    "- Dropped samples with the suffixes 'months' and 'cycles' from Adjuvant Chemo and Chemoradiation Duration cols\n",
    "- Dropped samples prefixed with '<' from the col CEA (ng/ml)\n",
    "- Dropped the single samples with the value 'Carcinoma in stiu' from Pre-Treatment T-Stage by MRI and ERUS\n",
    "- Fixed UTF-8 encoding error that represented the >= symbol with 'â‰¥', replaced with '>='\n",
    "- Dropped samples with the value 'Tumor deposits in subserosa/mesentery/non-peritonealized perirectal tissues' from Pre-Treatment N-Stage\n",
    "- Dropped samples with the value 'â‰¥10-19 yearsNot applicable years' from Crohn's Disease\n",
    "- Dropped sample with the value '4' in Weight Loss\n",
    "- Dropped samples with Database ID 'UP0272' and 'VU025', date of diagnosis was not a fully formated date and was causing errors\n",
    "- Corrected misspelling 'Tumor invades through muscularis propia'\n",
    "- Changed 'Tumor invades muscularis propria' to 'Tumor invades through muscularis propria' (UNDONE)\n",
    "- Dropped samples with the value 'T0N0' from Pre-Treatment AJCC Staging\n",
    "- Dropped the lone sample with the value '4-6 regional lymph nodes' in Pre-Treatment N-Stage by MRI\n",
    "- Dropped all samples prefixed with '>' and 'greater than' from columns indicating the Number of Mesorectal/Retroperitoneal/Pelvic Nodes in the pre-treatment section\n",
    "- Changed 'None' values to 'No' in col Ureteral Obstruction\n",
    "- Dropped the two samples with 'Boost dose' values in Radiation Dose\n",
    "- Dropped all samples prefixed with '>' in Number of Mesorectal/Retroperitoneal/Pelvic Nodes in the post-treatment section\n",
    "- Dropped the single '>0.44' value from Post-Treatment Tumor Diameter (cm)\n",
    "- Dropped the single '>1' value from # Lung Lesions on Imaging in Post-Treatment\n",
    "- Changed the single 'Liver metastases' value to 'Synchronous liver metastases' in CURRENT Tumor Category\n",
    "- Changed 'Total proctocolectomy' to 'APR' and dropped samples with 'Colostomy', 'Partial hepatectomy', and 'EMR' in Type of Operation of Rectal Tumor\n",
    "- Changed 'No reason stated' values to 'Other' in Reason for Conversion to Open\n",
    "- Dropped two samples with value 'â‰¥3' from Number of Staple Fires to Transect Rectum\n",
    "- In col Portion of Sphincters Resected, Defect Closure, Peritoneal Perforation, Positive Margin Requiring Re-excision, Need for Radical Resection, Intervention Required, Ostomy Reversed after Resolution of Leak, and Radiation changed 0 to 'No' and 1 to 'Yes'\n",
    "- Dropped samples with the value '>10' in # Liver Lesions in the Liver Resection section\n",
    "- Dropped single sample with the value 'Carcinoma in situ' from Post-Treatment T-Stage by ERUS\n",
    "- Dropped all samples with the value '4' in Tumor Differentiation\n",
    "- Dropped all samples prefixed with '>' or '<' from Proximal Margin Distance (cm), Distal Margin Distance (cm), and Radial Margin Distance (mm)\n",
    "- Changed '1' values to '1 regional lymph node' and fixed utf-8 encoding error for >= symbol in N-Stage.1\n",
    "- Dropped single '1' value from Type of Operation of Rectal Tumor\n",
    "- Dropped sample with Database ID 'UP0072', Anastonomic Leak diagnosis date was 7/11/01. Probably a typo for 2011, but better to drop just in case\n",
    "- Dropped samples with the value '3' in Method of Leak Diagnosis\n",
    "- Dropped samples with values '1', '2', and '3' in Type of Intervention\n",
    "- Fixed encoding error in Time to Readmission and Time to Death, â‰¤ to <=\n",
    "- Dropped single sample with the value 'UnkNown' in Adjuvant Chemoradiation\n",
    "- Dropped samples with values of 1433.2 and 1436.1 in Recurrence Free Survival (Months)\n",
    "- Dropped values not containing 'adenocarcinoma' from Histopathologic Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling outliers\n",
    "In numeric columns, outliers can signifigantly impare the function of a model. However, it can also be beneficial for the model to see that a value is higher than average. \n",
    "### Brainstorming\n",
    "The first thing we need to do is filter out any non-numeric data, thanfully this will now be easy since we went through and cleaned out dtypes already. Ideally, I would like to create a folder containing data on each feature. I would like to use sklearn to find the 5-10 most related features and build graphs for each via seaborn's pairplot. Alternatively, we could simply create a large pairplot for each feature, acting as some form of a heat map.\n",
    "#### Given that the methods to adjust outliers vary in end result given their application (regression or classification), it may be worth exploring implementing outlier adjustments in the dynamic learning process. Ideally, the method of fixing outliers will change based on whether the data is currently being used for regression or classification. This could also be an opprotunity to implement Minkowski error\n",
    "### End Approach\n",
    "Having read a few whitepapers that indicate the most effective methods of outlier detection and adjustment vary by whether or not the application is regression or classification. Since the proposed archetecture contains both, it would be un-ideal to implement just one for the ground truth dataset. Therefore we will focus more on the second proposal in the brainstorming section, implementing outlier correction on the fly during training. However, we will be creating two pairplots of features so anyone outside of this use case can visualize distributions and outliers of this cleaned version of the dataset for their own application.\n",
    "#### Had to scrap pair grids, cell would not execute. Commented out code in case someone would like to borrow it\n",
    "### Proposed pairplots\n",
    "1. Box and Scatter plots \n",
    "2. Distribution and Bivariate kde plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box_scat = sns.PairGrid(df, dropna=True)\n",
    "#box_scat = box_scat.map_diag(sns.boxplot, color = \"o\", orient = \"v\")\n",
    "#box_scat = box_scat.map_offdiag(sns.scatter)\n",
    "#box_scat.savefig(\"grid_boxandscat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def hexabin(x, y):\n",
    "#    plt.hexbin(x, y, gridsize=50, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distr_hex = sns.PairGrid(df, dropna=True)\n",
    "#distr_hex = distr_hex.map_diag(sns.distplot, color = \"o\")\n",
    "#distr_hex = distr_hex.map_offdiag(hexabin)\n",
    "#istr_hex.savefig(\"grid_distrandhex.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Imputation\n",
    "This dataset is incredibly dirty and has a lot of missing values, this means we have to impute, or fill in, these missing values. However the method of which we do this depends quite heavily on the specific feature and the data within, this means the dataset has to be gone over by hand. \n",
    "### Process:\n",
    "The approach I will be taking here will be very similar to binning. The features will be split up into dictionaries to impute a defined default value, or a conditional value determined by a related feature. All remaining empty values will be filled by MICE, this is in lew of simply selecting the median or maximum occurred values from the column as such practices are less than ideal given they can be less accurate and even introduce bias into the data. MICE is done last since it works best with MAR data.\n",
    "### Default values for categorical cols:\n",
    "Typically a value such as 'None' would be imputed uniformly if a categorical column (with multiple categories, ie not simply yes and no) does not apply to a sample. However, this dataset uses many different NA values which means we need to locate these and replace them with a uniform alternative so these columns will be easier to work with when handling things like conditional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcc-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
